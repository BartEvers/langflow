{
  "kind": "component",
  "type": "CleanlabRAGEvaluator",
  "display_name": "Cleanlab RAG Evaluator",
  "description": "Evaluates context, query, and response from a RAG pipeline using Cleanlab and outputs trust metrics.",
  "documentation": null,
  "icon": "Cleanlab",
  "minimized": null,
  "tags": [],
  "inputs": [
    {
      "field_class": "SecretStrInput",
      "name": "api_key",
      "display_name": "Cleanlab API Key",
      "info": "Your Cleanlab API key.",
      "required": true
    },
    {
      "field_class": "DropdownInput",
      "name": "model",
      "display_name": "Cleanlab Evaluation Model",
      "options": [
        "gpt-4.1",
        "gpt-4.1-mini",
        "gpt-4.1-nano",
        "o4-mini",
        "o3",
        "gpt-4.5-preview",
        "gpt-4o-mini",
        "gpt-4o",
        "o3-mini",
        "o1",
        "o1-mini",
        "gpt-4",
        "gpt-3.5-turbo-16k",
        "claude-3.7-sonnet",
        "claude-3.5-sonnet-v2",
        "claude-3.5-sonnet",
        "claude-3.5-haiku",
        "claude-3-haiku",
        "nova-micro",
        "nova-lite",
        "nova-pro"
      ],
      "info": "The model Cleanlab uses to evaluate the context, query, and response. This does NOT need to be the same model that generated the response.",
      "value": "gpt-4o-mini",
      "required": true,
      "advanced": true,
      "default": "gpt-4o-mini"
    },
    {
      "field_class": "DropdownInput",
      "name": "quality_preset",
      "display_name": "Quality Preset",
      "options": [
        "base",
        "low",
        "medium"
      ],
      "value": "medium",
      "info": "This determines the accuracy, latency, and cost of the evaluation. Higher quality is generally slower but more accurate.",
      "required": true,
      "advanced": true,
      "default": "medium"
    },
    {
      "field_class": "MessageTextInput",
      "name": "context",
      "display_name": "Context",
      "info": "The context retrieved for the given query.",
      "required": true
    },
    {
      "field_class": "MessageTextInput",
      "name": "query",
      "display_name": "Query",
      "info": "The user's query.",
      "required": true
    },
    {
      "field_class": "MessageTextInput",
      "name": "response",
      "display_name": "Response",
      "info": "The response generated by the LLM.",
      "required": true
    },
    {
      "field_class": "BoolInput",
      "name": "run_context_sufficiency",
      "display_name": "Run Context Sufficiency",
      "value": false,
      "advanced": true,
      "default": false
    },
    {
      "field_class": "BoolInput",
      "name": "run_response_groundedness",
      "display_name": "Run Response Groundedness",
      "value": false,
      "advanced": true,
      "default": false
    },
    {
      "field_class": "BoolInput",
      "name": "run_response_helpfulness",
      "display_name": "Run Response Helpfulness",
      "value": false,
      "advanced": true,
      "default": false
    },
    {
      "field_class": "BoolInput",
      "name": "run_query_ease",
      "display_name": "Run Query Ease",
      "value": false,
      "advanced": true,
      "default": false
    }
  ],
  "outputs": [
    {
      "output_class": "Output",
      "display_name": "Response",
      "name": "response_passthrough",
      "method": "pass_response",
      "types": [
        "Message"
      ]
    },
    {
      "output_class": "Output",
      "display_name": "Trust Score",
      "name": "trust_score",
      "method": "get_trust_score",
      "types": [
        "number"
      ]
    },
    {
      "output_class": "Output",
      "display_name": "Explanation",
      "name": "trust_explanation",
      "method": "get_trust_explanation",
      "types": [
        "Message"
      ]
    },
    {
      "output_class": "Output",
      "display_name": "Other Evals",
      "name": "other_scores",
      "method": "get_other_scores",
      "types": [
        "Data"
      ]
    },
    {
      "output_class": "Output",
      "display_name": "Evaluation Summary",
      "name": "evaluation_summary",
      "method": "get_evaluation_summary",
      "types": [
        "Message"
      ]
    }
  ],
  "template": [
    "api_key",
    "model",
    "quality_preset",
    "context",
    "query",
    "response",
    "run_context_sufficiency",
    "run_response_groundedness",
    "run_response_helpfulness",
    "run_query_ease"
  ],
  "examples": [
    {
      "op": "add_component",
      "type": "CleanlabRAGEvaluator"
    }
  ],
  "source": "src/backend/base/langflow/components/cleanlab/cleanlab_rag_evaluator.py#L14"
}