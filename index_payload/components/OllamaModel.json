{
  "kind": "component",
  "type": "OllamaModel",
  "display_name": "Ollama",
  "description": "Generate text using Ollama Local LLMs.",
  "documentation": null,
  "icon": "Ollama",
  "minimized": null,
  "tags": [],
  "inputs": [
    {
      "field_class": "MessageTextInput",
      "name": "base_url",
      "display_name": "Base URL",
      "info": "Endpoint of the Ollama API.",
      "value": "",
      "default": ""
    },
    {
      "field_class": "DropdownInput",
      "name": "model_name",
      "display_name": "Model Name",
      "options": [],
      "info": "Refer to https://ollama.com/library for more models.",
      "refresh_button": true,
      "real_time_refresh": true
    },
    {
      "field_class": "SliderInput",
      "name": "temperature",
      "display_name": "Temperature",
      "value": 0.1,
      "range_spec": "RangeSpec(min=0, max=1, step=0.01)",
      "advanced": true,
      "default": 0.1
    },
    {
      "field_class": "MessageTextInput",
      "name": "format",
      "display_name": "Format",
      "info": "Specify the format of the output (e.g., json).",
      "advanced": true
    },
    {
      "field_class": "DictInput",
      "name": "metadata",
      "display_name": "Metadata",
      "info": "Metadata to add to the run trace.",
      "advanced": true
    },
    {
      "field_class": "DropdownInput",
      "name": "mirostat",
      "display_name": "Mirostat",
      "options": [
        "Disabled",
        "Mirostat",
        "Mirostat 2.0"
      ],
      "info": "Enable/disable Mirostat sampling for controlling perplexity.",
      "value": "Disabled",
      "advanced": true,
      "real_time_refresh": true,
      "default": "Disabled"
    },
    {
      "field_class": "FloatInput",
      "name": "mirostat_eta",
      "display_name": "Mirostat Eta",
      "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
      "advanced": true
    },
    {
      "field_class": "FloatInput",
      "name": "mirostat_tau",
      "display_name": "Mirostat Tau",
      "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
      "advanced": true
    },
    {
      "field_class": "IntInput",
      "name": "num_ctx",
      "display_name": "Context Window Size",
      "info": "Size of the context window for generating tokens. (Default: 2048)",
      "advanced": true
    },
    {
      "field_class": "IntInput",
      "name": "num_gpu",
      "display_name": "Number of GPUs",
      "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
      "advanced": true
    },
    {
      "field_class": "IntInput",
      "name": "num_thread",
      "display_name": "Number of Threads",
      "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
      "advanced": true
    },
    {
      "field_class": "IntInput",
      "name": "repeat_last_n",
      "display_name": "Repeat Last N",
      "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
      "advanced": true
    },
    {
      "field_class": "FloatInput",
      "name": "repeat_penalty",
      "display_name": "Repeat Penalty",
      "info": "Penalty for repetitions in generated text. (Default: 1.1)",
      "advanced": true
    },
    {
      "field_class": "FloatInput",
      "name": "tfs_z",
      "display_name": "TFS Z",
      "info": "Tail free sampling value. (Default: 1)",
      "advanced": true
    },
    {
      "field_class": "IntInput",
      "name": "timeout",
      "display_name": "Timeout",
      "info": "Timeout for the request stream.",
      "advanced": true
    },
    {
      "field_class": "IntInput",
      "name": "top_k",
      "display_name": "Top K",
      "info": "Limits token selection to top K. (Default: 40)",
      "advanced": true
    },
    {
      "field_class": "FloatInput",
      "name": "top_p",
      "display_name": "Top P",
      "info": "Works together with top-k. (Default: 0.9)",
      "advanced": true
    },
    {
      "field_class": "BoolInput",
      "name": "verbose",
      "display_name": "Verbose",
      "info": "Whether to print out response text.",
      "advanced": true
    },
    {
      "field_class": "MessageTextInput",
      "name": "tags",
      "display_name": "Tags",
      "info": "Comma-separated list of tags to add to the run trace.",
      "advanced": true
    },
    {
      "field_class": "MessageTextInput",
      "name": "stop_tokens",
      "display_name": "Stop Tokens",
      "info": "Comma-separated list of tokens to signal the model to stop generating text.",
      "advanced": true
    },
    {
      "field_class": "MessageTextInput",
      "name": "system",
      "display_name": "System",
      "info": "System to use for generating text.",
      "advanced": true
    },
    {
      "field_class": "BoolInput",
      "name": "tool_model_enabled",
      "display_name": "Tool Model Enabled",
      "info": "Whether to enable tool calling in the model.",
      "value": true,
      "real_time_refresh": true,
      "default": true
    },
    {
      "field_class": "MessageTextInput",
      "name": "template",
      "display_name": "Template",
      "info": "Template to use for generating text.",
      "advanced": true
    },
    {
      "field_class": "Starred"
    }
  ],
  "outputs": [],
  "template": [
    "base_url",
    "model_name",
    "temperature",
    "format",
    "metadata",
    "mirostat",
    "mirostat_eta",
    "mirostat_tau",
    "num_ctx",
    "num_gpu",
    "num_thread",
    "repeat_last_n",
    "repeat_penalty",
    "tfs_z",
    "timeout",
    "top_k",
    "top_p",
    "verbose",
    "tags",
    "stop_tokens",
    "system",
    "tool_model_enabled",
    "template"
  ],
  "examples": [
    {
      "op": "add_component",
      "type": "OllamaModel"
    }
  ],
  "source": "src/backend/base/langflow/components/ollama/ollama.py#L18"
}